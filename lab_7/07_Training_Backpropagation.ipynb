{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: How to train your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks must be trained with (loads of) labelled data before they can produce more than just random noise. Once we have compiled our training data, the established training algorithm works as follows:\n",
    "\n",
    "- Feed-forward a member of the training set\n",
    "- Calculate the loss, i.e. the difference between the target output and the actual output\n",
    "- Back-propagate the loss into the network to determine each weight's contribution to the total loss\n",
    "- Apply an optimization algorithm (e.g. gradient-descend) to modify the network weights\n",
    "\n",
    "In our custom neural network, the task is carried out by the method ``train`` that takes a training image and a target vector as input:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "    def train(self, imgArr, target):\n",
    "        # Transform the image to a vector\n",
    "        inputs = imgArr.flatten()\n",
    "        # Move signal into hidden layer\n",
    "        hiddenInputs = np.dot(self.wih, inputs)\n",
    "        # Apply the activation function\n",
    "        hiddenOutputs = self.actFunc(hiddenInputs)\n",
    "        # Move signal into output layer\n",
    "        outputs = np.dot(self.who, hiddenOutputs)\n",
    "        # Apply the activation function\n",
    "        prediction = self.actFunc(outputs)\n",
    "        # output layer error is (target-actual)\n",
    "        outputErrors = target - prediction\n",
    "        # for the hidden layer error, we need to invert who\n",
    "        whoT = self.who.T\n",
    "        whoSq = np.dot(self.who,whoT)\n",
    "        whoInv = np.linalg.inv(whoSq)\n",
    "        whoInv = np.dot(whoT,whoInv)\n",
    "        hiddenErrors = np.dot(whoInv, outputErrors)\n",
    "        # update the weights between the hidden and output layer\n",
    "        err = outputErrors*prediction*(1.-prediction)\n",
    "        self.who += self.lRate * np.dot(err[:,np.newaxis], hiddenOutputs[np.newaxis,:])\n",
    "        # update the weights between the input and the hidden layer\n",
    "        err = hiddenErrors * hiddenOutputs * (1.-hiddenOutputs)\n",
    "        self.wih += self.lRate * np.dot(err[:,np.newaxis], inputs[np.newaxis,:])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed a training image into the network. For that, we first need to create an instance of the network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input nodes: 784 \n",
      "Hidden nodes: 100 \n",
      "Output nodes: 10 \n",
      "Learning rate: 0.6 \n",
      "wih matrix shape: (100, 784) \n",
      "who matrix shape: (10, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from network import neuralNetwork\n",
    "\n",
    "iNodes = 784 # The images are 28x28 pixels\n",
    "hNodes = 100 # An educated guess\n",
    "oNodes = 10 # Ten digits\n",
    "\n",
    "lRate = 0.6\n",
    "\n",
    "testNet = neuralNetwork(iNodes, hNodes, oNodes, lRate) # Create an instance of the network\n",
    "\n",
    "print(testNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the test image and label arrays back into our notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28) float64\n",
      "(10000,) int32\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.32941176 0.7254902  0.62352941 0.59215686 0.23529412 0.14117647\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.87058824 0.99607843 0.99607843 0.99607843 0.99607843 0.94509804\n",
      "  0.77647059 0.77647059 0.77647059 0.77647059 0.77647059 0.77647059\n",
      "  0.77647059 0.77647059 0.66666667 0.20392157 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.2627451  0.44705882 0.28235294 0.44705882 0.63921569 0.89019608\n",
      "  0.99607843 0.88235294 0.99607843 0.99607843 0.99607843 0.98039216\n",
      "  0.89803922 0.99607843 0.99607843 0.54901961 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.06666667\n",
      "  0.25882353 0.05490196 0.2627451  0.2627451  0.2627451  0.23137255\n",
      "  0.08235294 0.9254902  0.99607843 0.41568627 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3254902  0.99215686 0.81960784 0.07058824 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.08627451\n",
      "  0.91372549 1.         0.3254902  0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.50588235\n",
      "  0.99607843 0.93333333 0.17254902 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.23137255 0.97647059\n",
      "  0.99607843 0.24313725 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.52156863 0.99607843\n",
      "  0.73333333 0.01960784 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.03529412 0.80392157 0.97254902\n",
      "  0.22745098 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.49411765 0.99607843 0.71372549\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.29411765 0.98431373 0.94117647 0.22352941\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.0745098  0.86666667 0.99607843 0.65098039 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.79607843 0.99607843 0.85882353 0.1372549  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.14901961 0.99607843 0.99607843 0.30196078 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.12156863\n",
      "  0.87843137 0.99607843 0.45098039 0.00392157 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.52156863\n",
      "  0.99607843 0.99607843 0.20392157 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.23921569 0.94901961\n",
      "  0.99607843 0.99607843 0.20392157 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.4745098  0.99607843\n",
      "  0.99607843 0.85882353 0.15686275 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.4745098  0.99607843\n",
      "  0.81176471 0.07058824 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"mnistDataTest.npy\")\n",
    "labels = np.load(\"mnistLabelsTest.npy\")\n",
    "n = len(labels)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(labels.shape, labels.dtype)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tell the network which output we expect, we need to prepare the target vector. As we have ten output nodes, our target vector needs ten entries. In an ideal world, a perfect network would produce a perfect output consisting only of zeros and a single one at the position given by the label. For instance, if we feed in an image of the digit 2, we would love to see an output like\n",
    "\n",
    "``[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]``\n",
    "\n",
    "but because of certain similarities all digits share (certain pixels are black in almost every image), we introduce a tiny error margin by setting the target vector to\n",
    "\n",
    "``[0.01, 0.01, 0.99, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 # Index of some image (to avoid magic numbers ...)\n",
    "lbl = labels[idx] # Get the label of the first image\n",
    "\n",
    "target = np.zeros(10, dtype='float') + 0.01 # Set the target vector\n",
    "target[lbl] = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding in images into the network, we need to make sure that our data is as economic as possible. For a neural network, every value that is larger than 0 is considered as signal that must be learned and depending on the image format, 255 may be white and 0 black. Here, the data has already been optimised to account for signal economy, but at times it may be necessary to invert the images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for i in range(n):\n",
    "    data[i,:,:] = 1.-data[i,:,:]   \n",
    "print(data[0,:,:])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All what's left is a single line of code to feed in an image and improve the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNet.train(data[idx,:,:], target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, a single image does not suffice. Instead, we need to train the network with thousands of handwritten images and their respective labels to get reasonable results - a task that has been outsourced to the problem sheet :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
